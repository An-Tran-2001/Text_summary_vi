{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Demo cách làm vector Doc2vec ko làm luôn vì máy cùi</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import warnings\n",
    "from pyvi import ViTokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load the data\n",
    "path = os.getcwd() + '/../Data/demo-full.txt'\n",
    "sentences = open(path, 'r', encoding='utf-8').read()\n",
    "stop_words = open(os.getcwd() + '/../Data/stop_word/vietnamese-stopwords.txt', 'r', encoding='utf-8').read()\n",
    "stop_words = stop_words.split()\n",
    "# sentences[:100]\n",
    "# with open(path, 'r', encoding='utf-8') as f:\n",
    "#     sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_processing(sentences):\n",
    "    sentences = sentences.lower()\n",
    "    sentences = ViTokenizer.tokenize(sentences)\n",
    "    sentences = sentences.split('\\n')\n",
    "    # remove all the special characters\n",
    "    sentences = [re.sub(r'\\W', ' ', sentence) for sentence in sentences]\n",
    "    # remove all single characters\n",
    "    sentences = [re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence) for sentence in sentences]\n",
    "    # remove single characters from the start\n",
    "    sentences = [re.sub(r'\\^[a-zA-Z]\\s+', ' ', sentence) for sentence in sentences]\n",
    "    # remove numbers\n",
    "    sentences = [re.sub(r'\\d+', '', sentence) for sentence in sentences]\n",
    "    #  remove stop words\n",
    "    sentences = [[word for word in sentence.split() if word not in stop_words] for sentence in sentences]\n",
    "    #  remove words with length < 3\n",
    "    sentences = [[word for word in sentence if len(word) > 2] for sentence in sentences]\n",
    "    #  remove words with length > 15\n",
    "    sentences = [[word for word in sentence if len(word) < 15] for sentence in sentences]\n",
    "    #  remove empty sentences\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "    return sentences\n",
    "sentences = sentences_processing(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train the model\n",
    "skip_gram = Word2Vec(sentences, min_count=1, vector_size=100, window=3, sg=1)\n",
    "\n",
    "#  get the sentence vector/ lấy vector của câu\n",
    "def get_sentence_vector(sentence): #  get the sentence vector/ lấy vector của câu\n",
    "    sentence = [word for word in sentence if word not in stop_words] #  remove stop words/ loại bỏ stop words\n",
    "    sentence = [word for word in sentence if len(word) > 2] #  remove words with length < 3/ loại bỏ các từ có độ dài < 3\n",
    "    sentence = [word for word in sentence if len(word) < 15] #  remove words with length > 15/ loại bỏ các từ có độ dài > 15\n",
    "    # sentence = [word for word in sentence if word in skip_gram.wv.vocab]\n",
    "    #new gensnim 4.0.0\n",
    "    sentence = [word for word in sentence if word in list(skip_gram.wv.key_to_index.keys())] #  remove words not in the vocabulary/ loại bỏ các từ không có trong từ điển\n",
    "    if len(sentence) == 0: #  remove empty sentences/ loại bỏ các câu trống\n",
    "        return None #  return None if the sentence is empty/ trả về None nếu câu trống \n",
    "    return sum([skip_gram.wv[word] for word in sentence]) / len(sentence) #  return the average of all the word vectors in the sentence/ trả về trung bình của tất cả các vector từ trong câu\n",
    "\n",
    "#  get the similarity between two sentences\n",
    "def get_similarity(sentence1, sentence2): #  get the similarity between two sentences/ lấy độ tương tự giữa hai câu\n",
    "    sentence1 = get_sentence_vector(sentence1) #  get the sentence vector/ lấy vector của câu\n",
    "    sentence2 = get_sentence_vector(sentence2) #  get the sentence vector/ lấy vector của câu\n",
    "    if sentence1 is None or sentence2 is None: #  return 0 if one of the sentences is empty/ trả về 0 nếu một trong hai câu trống\n",
    "        return 0 #  return 0 if one of the sentences is empty/ trả về 0 nếu một trong hai câu trống\n",
    "    return np.dot(sentence1, sentence2) / (np.linalg.norm(sentence1) * np.linalg.norm(sentence2)) #  return the cosine similarity/ trả về độ tương tự cosine \n",
    "    # sentence1 = get_sentence_vector(sentence1)\n",
    "    # sentence2 = get_sentence_vector(sentence2)\n",
    "    # if sentence1 is None or sentence2 is None:\n",
    "    #     return 0\n",
    "    #     # axis 1 is out of bounds for array of dimension 1\n",
    "    \n",
    "    # return skip_gram.wv.cosine_similarities(sentence1, sentence2)[0]\n",
    "\n",
    "#  get the similarity matrix\n",
    "def get_similarity_matrix(sentences): #  get the similarity matrix/ lấy ma trận độ tương tự\n",
    "    n = len(sentences) #  get the number of sentences/ lấy số lượng câu\n",
    "    similarity_matrix = np.zeros((n, n)) #  create a matrix of zeros/ tạo ma trận 0\n",
    "    for i in range(n): #  loop through the sentences/ lặp qua các câu\n",
    "        for j in range(n): #  loop through the sentences/ lặp qua các câu\n",
    "            similarity_matrix[i][j] = get_similarity(sentences[i], sentences[j]) #  get the similarity between the two sentences/ lấy độ tương tự giữa hai câu\n",
    "    return similarity_matrix #  return the similarity matrix/ trả về ma trận độ tương tự\n",
    "\n",
    "#  get the similarity matrix\n",
    "similarity_matrix = get_similarity_matrix(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thường_trực thành_ủy hà_nội công_văn đồng_ý đề_xuất ban cán_sự đảng ubnd nội_tại báo_cáo bcs chủ_trương đầu_tư dự_án tổ_hợp thương_mại văn_phòng bán phường cầu diễn quận nam liêm hà_nội khu đất xuân hòa quản_lý trao_đổi phóng_viên báo đầu_tư bất_động_sản vấn_đề luật_sư trần đức phượng đoàn luật_sư hcm diễn tiến cổ_phần_hóa công_ty xuân_hòa quá_trình thay tên đổi chủ sử_dụng đất công_ty xuân hòa sang công_ty nhuệ giang đối_với diện_tích xuân hòa quản_lý có_thể khá giống trường_hợp xem thâu_tóm đất doanh_nghiệp nhà_nước cổ_phần_hóa thông_qua đấu_giá luật đất_đai kiểm_toán nhà_nước nêu trường_hợp như_cụ nguyễn thị thật thanh trì hà_nội cảnh chủ đầu_tư thu_hồi đất bồi_thường giấy_tờ đất tốn thật dân gửi đơn tòa_soạn phản_ánh đất_đai giải_quyết thấu tính đạt quy_định pháp_luật hiện_hành tòa cấp phúc_thẩm tỉnh lâm đồng nhận_định hai bản_án sơ_thẩm vi_phạm tố_tụng nội_dung buộc hủy đưa tòa sơ_thẩm_xét_xử nêu bảo_đảm quyền lợi_ích hợp_pháp dương thị cảnh tuổi ngụ đường nguyễn_thị thập phường tân phú quận chí minh tha trường_hợp chấp_nhận công_ty duy phát nộp tiền tha phương_án giải_quyết tiếp_theo báo cand đơn khiếu_nại khẩn_cấp cảnh quyền con_gái nguyễn thị hoàng vân tuổi thường_trú phường quận khiếu_nại chấp_hành viên cục thads tỉnh đồng tháp vi_phạm thủ_tục thads kéo_dài tha hồ_sơ con_gái cảnh công_ty duy phát nguyễn duy phương giám_đốc công_ty đại_diện mặt pháp_luật ký_kết giấy mượn tiền '"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  get the sentence scores/ lấy điểm câu\n",
    "def get_sentence_scores(similarity_matrix): #  get the sentence scores/ lấy điểm câu\n",
    "    sentence_scores = np.zeros(similarity_matrix.shape[0]) #  create a vector of zeros/ tạo vector 0\n",
    "    sentence_scores = np.sum(similarity_matrix, axis=1) #  get the sum of each row/ lấy tổng của mỗi hàng\n",
    "    return sentence_scores #  return the sentence scores/ trả về điểm câu\n",
    "\n",
    "#  get the sentence scores/ lấy điểm câu\n",
    "sentence_scores = get_sentence_scores(similarity_matrix) #  get the sentence scores/ lấy điểm câu\n",
    "\n",
    "#  get the summary/ lấy tóm tắt\n",
    "\n",
    "def get_summary(sentences, sentence_scores, top_n=5): #  get the summary/ lấy tóm tắt\n",
    "    top_sentence_indices = sentence_scores.argsort()[-top_n:][::-1] #  get the indices of the top n sentences/ lấy chỉ số của n câu hàng đầu\n",
    "    top_sentence_indices.sort() #  sort the indices/ sắp xếp các chỉ số\n",
    "    summary = [sentences[i] for i in top_sentence_indices] #  get the top n sentences/ lấy n câu hàng đầu\n",
    "\n",
    "    return ''.join([w + ' ' for s in summary for w in s])  #  return the summary/ trả về tóm tắt\n",
    "\n",
    "#  get the summary/ lấy tóm tắt\n",
    "summary = get_summary(sentences, sentence_scores, 5) #  get the summary/ lấy tóm tắt\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.68059564e-04,  1.55570311e-02,  3.00582871e-03, -3.45600373e-03,\n",
       "        2.58503645e-03, -2.29644459e-02,  6.56370912e-03,  4.59810384e-02,\n",
       "        6.02862984e-03,  6.62437174e-03, -2.29573827e-02, -2.00491920e-02,\n",
       "        3.20818555e-03,  7.57543696e-03,  1.31806117e-02, -1.62626524e-02,\n",
       "       -4.52424353e-03, -2.55763568e-02,  6.85427850e-03, -2.50079557e-02,\n",
       "        1.48634715e-02,  6.65149838e-03,  6.96866075e-03, -2.40111910e-02,\n",
       "       -7.21283443e-03,  1.37907779e-02, -2.02631811e-04, -1.95640624e-02,\n",
       "       -1.36458296e-02,  5.30999945e-03,  9.34937969e-03,  2.83040619e-03,\n",
       "        1.48273837e-02, -1.24605419e-02, -2.39533302e-03,  1.89499091e-02,\n",
       "        6.52748113e-03, -1.11439712e-02, -1.41496677e-02, -2.90083587e-02,\n",
       "        8.27956200e-03, -2.22081337e-02, -5.10681071e-04,  1.14688268e-02,\n",
       "        9.43624415e-03, -7.19758263e-03,  2.08234508e-03, -3.57544166e-03,\n",
       "        7.31441705e-03,  1.26361409e-02,  4.20154817e-03, -5.40881930e-03,\n",
       "       -1.21997138e-02, -5.58011187e-03, -8.89860559e-03,  1.39943259e-02,\n",
       "        1.18223904e-02,  6.48027612e-03, -1.59098487e-02, -7.86760449e-03,\n",
       "       -7.19413394e-03,  1.23575889e-02, -1.23029454e-02,  3.44212120e-03,\n",
       "       -2.84481570e-02,  2.53691599e-02,  9.68179724e-04,  4.75093629e-03,\n",
       "       -2.51172893e-02,  9.86898784e-03, -2.81509534e-02, -2.50541908e-03,\n",
       "        1.94656067e-02, -7.29153678e-03,  6.84012938e-03, -7.89144076e-03,\n",
       "       -1.18691269e-02,  7.18937721e-03, -1.17809055e-02,  1.00296624e-02,\n",
       "       -1.95561498e-02,  4.95002791e-03, -1.89249888e-02,  1.17354291e-02,\n",
       "       -2.61995359e-03,  7.27752829e-03,  1.17002781e-02,  1.65587980e-02,\n",
       "        1.71786156e-02, -2.06745532e-03,  9.33175813e-03,  3.36265774e-04,\n",
       "        1.35890632e-05,  5.17940894e-03,  3.26746218e-02,  3.60638881e-03,\n",
       "        1.62420992e-03, -1.54489593e-04,  1.11368001e-02,  2.96533317e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Save the model\n",
    "skip_gram.save('skip_gram.bin')\n",
    "#  Load the model\n",
    "skip_gram = Word2Vec.load('skip_gram.bin')\n",
    "#  get the \n",
    "skip_gram.wv['thủ_tướng']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('đường', 0.9853959083557129),\n",
       " ('thu', 0.9852155447006226),\n",
       " ('thọ', 0.9849787354469299),\n",
       " ('xử_phạt', 0.9848214387893677),\n",
       " ('thông_tin', 0.98479825258255),\n",
       " ('khu', 0.984673261642456),\n",
       " ('quỹ', 0.9846349358558655),\n",
       " ('vi_phạm', 0.9846035838127136),\n",
       " ('hai', 0.9844377636909485),\n",
       " ('trả', 0.9843767881393433)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram.wv.most_similar(positive=['tiền', 'thuế'], negative=['lương'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdfbf7b7304e05408e799db5ef32899d3a083826c8173734f898b7fad24dd3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
